<div align="center">

# AgentReward

**è¿‡ç¨‹çº§ Reward è®¡ç®—å¼•æ“ - è¯„ä¼° Agent ä¸ä»…åšå¯¹äº†ä»€ä¹ˆï¼Œè¿˜è¯„ä¼°æ€ä¹ˆåšçš„**
**Process-level rubric-based reward engine for Code Agent trajectories**

[![PyPI](https://img.shields.io/pypi/v/knowlyr-reward?color=blue)](https://pypi.org/project/knowlyr-reward/)
[![Python 3.10+](https://img.shields.io/badge/python-3.10+-blue.svg)](https://www.python.org/downloads/)
[![License: MIT](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)
[![MCP](https://img.shields.io/badge/MCP-4_Tools-purple.svg)](#mcp-server)

[å¿«é€Ÿå¼€å§‹](#å¿«é€Ÿå¼€å§‹) Â· [ä¸‰å±‚æ¶æ„](#ä¸‰å±‚æ¶æ„) Â· [Rubric ä½“ç³»](#rubric-ä½“ç³») Â· [MCP Server](#mcp-server) Â· [Data Pipeline ç”Ÿæ€](#data-pipeline-ç”Ÿæ€)

</div>

---

**GitHub Topics**: `agent-reward`, `process-reward`, `rubric`, `llm-judge`, `rlhf`, `code-agent`

å¯¹ Agent è½¨è¿¹çš„æ¯ä¸€æ­¥è®¡ç®—å¤šç»´ Rubric Rewardï¼Œæ”¯æŒè§„åˆ™å±‚ + æ¨¡å‹å±‚ + äººå·¥æ ¡å‡†ã€‚ç”¨äº RLHF/DPO è®­ç»ƒæ•°æ®çš„åå¥½å¯¹æ„å»ºã€‚

## æ ¸å¿ƒèƒ½åŠ› / Core Capabilities

```
Agent è½¨è¿¹ (Næ­¥) â†’ é€æ­¥è¯„ä¼° â†’ è¿‡ç¨‹åˆ† + ç»“æœåˆ† â†’ åå¥½å¯¹ â†’ RLHF/DPO è®­ç»ƒ
```

### è§£å†³çš„é—®é¢˜ / Problems Solved

| ç—›ç‚¹ | ä¼ ç»Ÿæ–¹æ¡ˆ | AgentReward |
|------|----------|-------------|
| **è¯„ä¼°ç²’åº¦** | åªçœ‹æœ€ç»ˆç»“æœ pass/fail | æ¯ä¸€æ­¥éƒ½æœ‰å¤šç»´åˆ†æ•° |
| **Reward ä¿¡å·** | ç¨€ç– (0/1) | å¯†é›† (æ¯æ­¥ 0.0-1.0) |
| **å¯è§£é‡Šæ€§** | é»‘ç›’åˆ†æ•° | æŒ‰ Rubric æ‹†è§£ + ç†ç”± |
| **åå¥½æ„å»º** | æ‰‹åŠ¨æ ‡æ³¨ | è‡ªåŠ¨ä» Reward æ’åºç”Ÿæˆ |
| **å¯é æ€§** | çº¯ LLM åˆ¤æ–­ä¸ç¨³å®š | è§„åˆ™å…œåº• + æ¨¡å‹å¢å¼º + äººå·¥æ ¡å‡† |

## å®‰è£… / Installation

```bash
pip install knowlyr-reward
```

å¯é€‰ä¾èµ–ï¼š

```bash
pip install knowlyr-reward[llm]    # LLM-as-Judge (Anthropic + OpenAI)
pip install knowlyr-reward[stats]  # ç»Ÿè®¡æ ¡å‡† (numpy + scipy)
pip install knowlyr-reward[mcp]    # MCP æœåŠ¡å™¨
pip install knowlyr-reward[all]    # å…¨éƒ¨åŠŸèƒ½
```

## å¿«é€Ÿå¼€å§‹ / Quick Start

### Python API

```python
from agentreward import RewardEngine, TrajectoryReward
from agentreward.config import RewardConfig

# å‡†å¤‡è½¨è¿¹æ•°æ®
trajectory = {
    "task": "ä¿®å¤ test_login.py ä¸­çš„æ–­è¨€é”™è¯¯",
    "steps": [
        {"tool": "Read", "params": {"file_path": "/src/test_login.py"}, "output": "..."},
        {"tool": "Grep", "params": {"pattern": "assert"}, "output": "line 42: assert x == y"},
        {"tool": "Edit", "params": {"file_path": "/src/test_login.py",
                                     "old_string": "assert x == y",
                                     "new_string": "assert x == expected_y"}},
    ],
    "outcome": {"success": True, "tests_passed": 10, "tests_total": 10},
}

# è®¡ç®— Reward
engine = RewardEngine()
result = engine.score(trajectory)

print(f"æ€»åˆ†: {result.total_score:.4f}")
print(f"ç»“æœåˆ†: {result.outcome_score:.4f}")
print(f"è¿‡ç¨‹åˆ†: {result.process_score:.4f}")

for sr in result.step_rewards:
    print(f"  Step {sr.step_id}: {sr.total_score:.4f} {sr.rubric_scores}")
```

<details>
<summary>è¾“å‡ºç¤ºä¾‹</summary>

```
æ€»åˆ†: 0.8720
ç»“æœåˆ†: 1.0000
è¿‡ç¨‹åˆ†: 0.7440
  Step 1: 0.8500 {'goal_progress': 0.8, 'tool_choice': 0.9, 'param_correctness': 0.9, 'info_utilization': 0.7, 'non_redundancy': 1.0}
  Step 2: 0.7200 {'goal_progress': 0.6, 'tool_choice': 0.8, 'param_correctness': 0.8, 'info_utilization': 0.6, 'non_redundancy': 0.9}
  Step 3: 0.9100 {'goal_progress': 0.9, 'tool_choice': 1.0, 'param_correctness': 0.9, 'info_utilization': 0.9, 'non_redundancy': 1.0}
```

</details>

### CLI å‘½ä»¤è¡Œ

```bash
# è¯„ä¼°å•æ¡è½¨è¿¹
knowlyr-reward score trajectory.json

# æ¯”è¾ƒå¤šæ¡è½¨è¿¹
knowlyr-reward compare traj_a.json traj_b.json traj_c.json

# æ„å»ºåå¥½å¯¹
knowlyr-reward preferences trajectories_by_task.json -o pairs.json
```

<details>
<summary>è¾“å‡ºç¤ºä¾‹</summary>

```
æ­£åœ¨è¯„ä¼°è½¨è¿¹: trajectory.json
  æ­¥éª¤æ•°: 5
  æ¨¡å‹: claude-sonnet-4-20250514
  è¿›åº¦: 5/5
âœ“ è¯„ä¼°å®Œæˆ
  æ€»åˆ†: 0.8720
  è¿‡ç¨‹åˆ†: 0.7440
  ç»“æœåˆ†: 1.0000
  è€—æ—¶: 3.2s
```

</details>

---

## ä¸‰å±‚æ¶æ„ / Three-Layer Architecture

```mermaid
graph TD
    subgraph L1["Layer 1 Â· è§„åˆ™å±‚ (æƒé‡ 0.6)"]
        direction TB
        R1["Rule-based"]
        R1a["å†—ä½™æ£€æµ‹ Â· å›é€€æ£€æµ‹<br/>æ•ˆç‡è®¡ç®— Â· ä¿¡æ¯åˆ©ç”¨"]
        R1b["âœ… ç¡®å®šæ€§ã€å¿«é€Ÿã€æ— éœ€ API"]
    end

    subgraph L2["Layer 2 Â· æ¨¡å‹å±‚ (æƒé‡ 0.4)"]
        direction TB
        R2["LLM-as-Judge"]
        R2a["ç›®æ ‡æ¨è¿›è¯„ä¼° Â· å·¥å…·é€‰æ‹©è¯„ä¼°<br/>å‚æ•°æ­£ç¡®æ€§è¯„ä¼° Â· Prompt æ¨¡æ¿"]
        R2b["ğŸ§  è¯­ä¹‰ç†è§£ã€çµæ´»ã€éœ€è¦ LLM API"]
    end

    subgraph L3["Layer 3 Â· äººå·¥æ ¡å‡†"]
        direction TB
        R3["Human Calibration"]
        R3a["Pearson/Spearman Â· ä¸€è‡´ç‡è®¡ç®—<br/>æƒé‡è°ƒä¼˜ Â· MAE åˆ†æ"]
        R3b["ğŸ‘¤ å¯é æ€§ä¿è¯ã€éœ€è¦äººå·¥æ ‡æ³¨"]
    end

    L1 --> Merge["ğŸ¯ åŠ æƒèåˆ"]
    L2 --> Merge
    Merge --> L3

    style L1 fill:#2da44e,color:#fff,stroke:#2da44e
    style L2 fill:#0969da,color:#fff,stroke:#0969da
    style L3 fill:#8250df,color:#fff,stroke:#8250df
    style Merge fill:#bf8700,color:#fff,stroke:#bf8700
```

**ä¸ºä»€ä¹ˆéœ€è¦ä¸‰å±‚ï¼Ÿ**
- è§„åˆ™å±‚ï¼šå¿«é€Ÿã€ç¡®å®šæ€§ã€é›¶æˆæœ¬ï¼Œè¦†ç›–å¯é‡åŒ–çš„ç»´åº¦ï¼ˆå†—ä½™ã€å›é€€ã€æ•ˆç‡ï¼‰
- æ¨¡å‹å±‚ï¼šç†è§£è¯­ä¹‰ï¼Œè¯„ä¼°"ç›®æ ‡æ¨è¿›"ç­‰éœ€è¦ç†è§£èƒ½åŠ›çš„ç»´åº¦
- äººå·¥å±‚ï¼šæ ¡å‡†å‰ä¸¤å±‚çš„è¾“å‡ºï¼Œç¡®ä¿ä¸äººç±»åˆ¤æ–­ä¸€è‡´

---

## Rubric ä½“ç³» / Rubric System

æ¯æ¡è½¨è¿¹çš„æ¯ä¸€æ­¥æŒ‰ 5 ä¸ªç»´åº¦è¯„ä¼°ï¼š

| Rubric | åç§° | æƒé‡ | è¯„ä¼°æ–¹å¼ | è¯´æ˜ |
|--------|------|------|----------|------|
| `goal_progress` | ç›®æ ‡æ¨è¿› | 0.30 | model | è¿™ä¸€æ­¥æ˜¯å¦æ¨è¿›äº†ä»»åŠ¡ç›®æ ‡ï¼Ÿ |
| `tool_choice` | å·¥å…·é€‰æ‹© | 0.20 | model | é€‰æ‹©çš„å·¥å…·æ˜¯å¦åˆç†ï¼Ÿ |
| `param_correctness` | å‚æ•°æ­£ç¡®æ€§ | 0.20 | model | å·¥å…·è°ƒç”¨çš„å‚æ•°æ˜¯å¦æ­£ç¡®ï¼Ÿ |
| `info_utilization` | ä¿¡æ¯åˆ©ç”¨ | 0.15 | rule | æ˜¯å¦åˆ©ç”¨äº†ä¹‹å‰è·å¾—çš„ä¿¡æ¯ï¼Ÿ |
| `non_redundancy` | éå†—ä½™æ€§ | 0.15 | rule | è¿™ä¸€æ­¥æ˜¯å¦æ˜¯éå†—ä½™æ“ä½œï¼Ÿ |

### è‡ªå®šä¹‰ Rubric

```python
from agentreward.rubrics import Rubric, RubricSet

custom_rubrics = RubricSet(rubrics=[
    Rubric(id="safety", name="å®‰å…¨æ€§", description="æ“ä½œæ˜¯å¦å®‰å…¨ï¼Ÿ",
           weight=0.4, evaluator="rule"),
    Rubric(id="creativity", name="åˆ›é€ æ€§", description="æ–¹æ¡ˆæ˜¯å¦æœ‰åˆ›æ„ï¼Ÿ",
           weight=0.6, evaluator="model"),
])
```

---

## æ ¡å‡†æ–¹æ³• / Calibration Methodology

æ ¡å‡†æµç¨‹ï¼š

1. **æ”¶é›†äººå·¥æ ‡æ³¨**: å¯¹ 50-100 æ¡è½¨è¿¹ç”±äººå·¥ä¸“å®¶è¯„åˆ†
2. **è®¡ç®—ç›¸å…³æ€§**: Pearson r (çº¿æ€§)ã€Spearman rho (æ’åº)ã€ä¸€è‡´ç‡
3. **è°ƒä¼˜æƒé‡**: æ ¹æ®ç›¸å…³æ€§ç»“æœè°ƒæ•´ rule_weight / model_weight
4. **è¿­ä»£**: é‡å¤ç›´åˆ° Spearman rho > 0.8

```python
from agentreward.calibration import calibrate

result = calibrate(
    reward_scores=[0.8, 0.6, 0.9, 0.3, 0.7],
    human_scores=[0.85, 0.55, 0.95, 0.25, 0.65],
)

print(f"Pearson r: {result.pearson_r:.4f}")
print(f"Spearman rho: {result.spearman_rho:.4f}")
print(f"Agreement rate: {result.agreement_rate:.4f}")
```

### æ ¡å‡†æŒ‡æ ‡å‚è€ƒ

| æŒ‡æ ‡ | åˆæ ¼ | è‰¯å¥½ | ä¼˜ç§€ |
|------|------|------|------|
| Pearson r | > 0.5 | > 0.7 | > 0.85 |
| Spearman rho | > 0.5 | > 0.7 | > 0.85 |
| Agreement rate | > 0.6 | > 0.75 | > 0.9 |

---

## åå¥½å¯¹æ„å»º / Preference Pair Construction

ç”¨äº RLHF / DPO è®­ç»ƒï¼š

```python
from agentreward.preferences import build_preferences

# æŒ‰ä»»åŠ¡åˆ†ç»„çš„è½¨è¿¹ (å·²å« reward åˆ†æ•°)
trajectories_by_task = {
    "task_001": [
        {"id": "traj_a", "reward": 0.9, "step_count": 5},
        {"id": "traj_b", "reward": 0.3, "step_count": 12},
        {"id": "traj_c", "reward": 0.7, "step_count": 8},
    ],
}

pairs = build_preferences(trajectories_by_task, min_margin=0.1)
for p in pairs:
    print(f"{p.chosen_trajectory_id} > {p.rejected_trajectory_id} (margin={p.margin():.3f})")
```

---

## MCP Server / Claude Integration

åœ¨ Claude Desktop / Claude Code ä¸­ç›´æ¥ä½¿ç”¨ã€‚

### é…ç½® / Config

æ·»åŠ åˆ° `~/Library/Application Support/Claude/claude_desktop_config.json`ï¼š

```json
{
  "mcpServers": {
    "knowlyr-reward": {
      "command": "uv",
      "args": ["--directory", "/path/to/agent-reward", "run", "python", "-m", "agentreward.mcp_server"]
    }
  }
}
```

### å¯ç”¨å·¥å…· / Tools

| å·¥å…· | åŠŸèƒ½ |
|------|------|
| `score_trajectory` | å¯¹å•æ¡è½¨è¿¹è®¡ç®—è¿‡ç¨‹çº§ Reward |
| `build_preferences` | ä»å¤šæ¡è½¨è¿¹æ„å»ºåå¥½å¯¹ |
| `calibrate_reward` | å°†è‡ªåŠ¨ Reward ä¸äººå·¥æ ‡æ³¨æ ¡å‡† |
| `list_rubrics` | åˆ—å‡ºå¯ç”¨çš„è¯„ä¼° Rubric |

### ä½¿ç”¨ç¤ºä¾‹ / Usage Example

```
ç”¨æˆ·: å¸®æˆ‘è¯„ä¼° ./trajectories/task_001.json çš„ Agent è½¨è¿¹

Claude: [è°ƒç”¨ score_trajectory]
        è¯„ä¼°è½¨è¿¹ (5 æ­¥)...

        âœ“ è¯„ä¼°å®Œæˆ:
        - æ€»åˆ†: 0.8720
        - è¿‡ç¨‹åˆ†: 0.7440
        - ç»“æœåˆ†: 1.0000
        - Step 1: 0.85 | Step 2: 0.72 | Step 3: 0.91
```

---

## Data Pipeline ç”Ÿæ€ / Ecosystem

AgentReward æ˜¯ Data Pipeline ç”Ÿæ€çš„ Reward ç»„ä»¶ï¼š

```mermaid
graph LR
    Radar["ğŸ” Radar<br/>æƒ…æŠ¥å‘ç°"] --> Recipe["ğŸ“‹ Recipe<br/>é€†å‘åˆ†æ"]
    Recipe --> Synth["ğŸ”„ Synth<br/>æ•°æ®åˆæˆ"]
    Recipe --> Label["ğŸ·ï¸ Label<br/>æ•°æ®æ ‡æ³¨"]
    Synth --> Check["âœ… Check<br/>æ•°æ®è´¨æ£€"]
    Label --> Check
    Check --> Audit["ğŸ”¬ Audit<br/>æ¨¡å‹å®¡è®¡"]
    Audit --> Hub["ğŸ¯ Hub<br/>ç¼–æ’å±‚"]
    Hub --> Sandbox["ğŸ“¦ Sandbox<br/>æ‰§è¡Œæ²™ç®±"]
    Sandbox --> Recorder["ğŸ“¹ Recorder<br/>è½¨è¿¹å½•åˆ¶"]
    Recorder --> Reward["â­ Reward<br/>è¿‡ç¨‹æ‰“åˆ†"]
    style Reward fill:#0969da,color:#fff,stroke:#0969da
```

### ç”Ÿæ€é¡¹ç›®

| å±‚ | é¡¹ç›® | è¯´æ˜ | ä»“åº“ |
|---|---|---|---|
| æƒ…æŠ¥ | **AI Dataset Radar** | æ•°æ®é›†ç«äº‰æƒ…æŠ¥ã€è¶‹åŠ¿åˆ†æ | [GitHub](https://github.com/liuxiaotong/ai-dataset-radar) |
| åˆ†æ | **DataRecipe** | é€†å‘åˆ†æã€Schema æå–ã€æˆæœ¬ä¼°ç®— | [GitHub](https://github.com/liuxiaotong/data-recipe) |
| ç”Ÿäº§ | **DataSynth** | LLM æ‰¹é‡åˆæˆã€ç§å­æ•°æ®æ‰©å…… | [GitHub](https://github.com/liuxiaotong/data-synth) |
| ç”Ÿäº§ | **DataLabel** | è½»é‡æ ‡æ³¨å·¥å…·ã€å¤šæ ‡æ³¨å‘˜åˆå¹¶ | [GitHub](https://github.com/liuxiaotong/data-label) |
| è´¨æ£€ | **DataCheck** | è§„åˆ™éªŒè¯ã€é‡å¤æ£€æµ‹ã€åˆ†å¸ƒåˆ†æ | [GitHub](https://github.com/liuxiaotong/data-check) |
| è´¨æ£€ | **ModelAudit** | è’¸é¦æ£€æµ‹ã€æ¨¡å‹æŒ‡çº¹ã€èº«ä»½éªŒè¯ | [GitHub](https://github.com/liuxiaotong/model-audit) |
| Agent | **AgentSandbox** | Docker æ‰§è¡Œæ²™ç®±ã€è½¨è¿¹é‡æ”¾ | [GitHub](https://github.com/liuxiaotong/agent-sandbox) |
| Agent | **AgentRecorder** | æ ‡å‡†åŒ–è½¨è¿¹å½•åˆ¶ã€å¤šæ¡†æ¶é€‚é… | [GitHub](https://github.com/liuxiaotong/agent-recorder) |
| Agent | **AgentReward** | è¿‡ç¨‹çº§ Rewardã€Rubric å¤šç»´è¯„ä¼° | You are here |
| ç¼–æ’ | **TrajectoryHub** | Pipeline ç¼–æ’ã€æ•°æ®é›†å¯¼å‡º | [GitHub](https://github.com/liuxiaotong/agent-trajectory-hub) |

### ç«¯åˆ°ç«¯å·¥ä½œæµ / End-to-end Flow

```bash
# 1. Radar: å‘ç°é«˜è´¨é‡æ•°æ®é›†
knowlyr-radar scan --domain code-agent

# 2. DataRecipe: åˆ†ææ•°æ®é›†ï¼Œç”Ÿæˆ Schema å’Œæ ·ä¾‹
knowlyr-datarecipe deep-analyze tencent/CL-bench -o ./output

# 3. DataSynth: åŸºäºç§å­æ•°æ®æ‰¹é‡åˆæˆ
knowlyr-datasynth generate ./output/tencent_CL-bench/ -n 1000

# 4. DataLabel: äººå·¥æ ‡æ³¨/æ ¡å‡†ç§å­æ•°æ®
knowlyr-datalabel generate ./output/tencent_CL-bench/

# 5. DataCheck: è´¨é‡æ£€æŸ¥
knowlyr-datacheck validate ./output/tencent_CL-bench/

# 6. Recorder: å½•åˆ¶ Agent æ‰§è¡Œè½¨è¿¹
knowlyr-recorder record --task task_001.json

# 7. Hub: ç®¡ç†è½¨è¿¹æ•°æ®
knowlyr-hub import ./trajectories/

# 8. Sandbox: å®‰å…¨å›æ”¾éªŒè¯
knowlyr-sandbox replay trajectory_001.json

# 9. AgentReward: è®¡ç®—è¿‡ç¨‹çº§ Reward + æ„å»ºåå¥½å¯¹
knowlyr-reward score trajectory_001.json
knowlyr-reward preferences trajectories_by_task.json -o pairs.json
```

### å…¨å®¶æ¡¶ MCP é…ç½® / Full MCP Config

```json
{
  "mcpServers": {
    "knowlyr-radar": {
      "command": "uv",
      "args": ["--directory", "/path/to/ai-dataset-radar", "run", "knowlyr-radar-mcp"]
    },
    "knowlyr-datarecipe": {
      "command": "uv",
      "args": ["--directory", "/path/to/data-recipe", "run", "knowlyr-datarecipe-mcp"]
    },
    "knowlyr-datasynth": {
      "command": "uv",
      "args": ["--directory", "/path/to/data-synth", "run", "python", "-m", "datasynth.mcp_server"]
    },
    "knowlyr-datalabel": {
      "command": "uv",
      "args": ["--directory", "/path/to/data-label", "run", "python", "-m", "datalabel.mcp_server"]
    },
    "knowlyr-datacheck": {
      "command": "uv",
      "args": ["--directory", "/path/to/data-check", "run", "python", "-m", "datacheck.mcp_server"]
    },
    "knowlyr-hub": {
      "command": "uv",
      "args": ["--directory", "/path/to/agent-trajectory-hub", "run", "python", "-m", "trajhub.mcp_server"]
    },
    "knowlyr-sandbox": {
      "command": "uv",
      "args": ["--directory", "/path/to/agent-sandbox", "run", "python", "-m", "sandbox.mcp_server"]
    },
    "knowlyr-recorder": {
      "command": "uv",
      "args": ["--directory", "/path/to/agent-recorder", "run", "python", "-m", "recorder.mcp_server"]
    },
    "knowlyr-reward": {
      "command": "uv",
      "args": ["--directory", "/path/to/agent-reward", "run", "python", "-m", "agentreward.mcp_server"]
    }
  }
}
```

---

## å‘½ä»¤å‚è€ƒ

| å‘½ä»¤ | åŠŸèƒ½ |
|------|------|
| `knowlyr-reward score <file>` | è¯„ä¼°å•æ¡è½¨è¿¹ |
| `knowlyr-reward compare <files...>` | æ¯”è¾ƒå¤šæ¡è½¨è¿¹ |
| `knowlyr-reward preferences <file>` | æ„å»ºåå¥½å¯¹ |
| `knowlyr-reward calibrate <file>` | äººå·¥æ ¡å‡† |
| `knowlyr-reward rubrics` | åˆ—å‡º Rubric |

---

## API ä½¿ç”¨

```python
from agentreward import RewardEngine
from agentreward.config import RewardConfig

# é…ç½®
config = RewardConfig(
    rule_weight=0.6,       # è§„åˆ™å±‚æƒé‡
    model_weight=0.4,      # æ¨¡å‹å±‚æƒé‡
    rubric_set="default",  # Rubric é›†åˆ
    model_name="claude-sonnet-4-20250514",
    provider="anthropic",
    temperature=0.1,
)

# è¯„ä¼°
engine = RewardEngine(config)
result = engine.score(trajectory)

print(f"æ€»åˆ†: {result.total_score:.4f}")
print(f"è¿‡ç¨‹åˆ†: {result.process_score:.4f}")
```

### Core Classes

| ç±» | è¯´æ˜ |
|---|------|
| `RewardEngine` | æ ¸å¿ƒå¼•æ“ï¼Œç»„åˆè§„åˆ™å±‚å’Œæ¨¡å‹å±‚ |
| `StepReward` | å•æ­¥ Reward ç»“æœ |
| `TrajectoryReward` | è½¨è¿¹ Reward ç»“æœ |
| `Rubric` | å•ä¸ªè¯„ä¼°ç»´åº¦ |
| `RubricSet` | è¯„ä¼°ç»´åº¦é›†åˆ |
| `PreferencePair` | åå¥½å¯¹ |
| `RewardConfig` | å¼•æ“é…ç½® |
| `CalibrationResult` | æ ¡å‡†ç»“æœ |

---

## é¡¹ç›®æ¶æ„

```
src/agentreward/
â”œâ”€â”€ reward.py        # æ ¸å¿ƒå¼•æ“ (RewardEngine)
â”œâ”€â”€ rubrics.py       # Rubric å®šä¹‰ (5 ä¸ªé»˜è®¤ç»´åº¦)
â”œâ”€â”€ rules.py         # è§„åˆ™å±‚ (å†—ä½™/å›é€€/æ•ˆç‡/ä¿¡æ¯åˆ©ç”¨)
â”œâ”€â”€ judge.py         # æ¨¡å‹å±‚ (LLM-as-Judge)
â”œâ”€â”€ preferences.py   # åå¥½å¯¹æ„å»º
â”œâ”€â”€ calibration.py   # äººå·¥æ ¡å‡†
â”œâ”€â”€ config.py        # é…ç½®
â”œâ”€â”€ cli.py           # CLI å‘½ä»¤è¡Œ
â””â”€â”€ mcp_server.py    # MCP Server (4 å·¥å…·)
```

---

## License

[MIT](LICENSE)

---

<div align="center">
<sub>è¯„ä¼° Agent ä¸ä»…åšå¯¹äº†ä»€ä¹ˆï¼Œè¿˜è¯„ä¼°æ€ä¹ˆåšçš„</sub>
</div>
